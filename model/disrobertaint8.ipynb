{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run this on local environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, Value, ClassLabel, Features\n",
    "\n",
    "# from optimum.intel.neural_compressor import INCModelForSequenceClassification\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "\n",
    "# model_id = \"Intel/distilbert-base-uncased-finetuned-sst-2-english-int8-static\"\n",
    "# int8_model = INCModelForSequenceClassification.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/75510487/huggingface-trainer-k-fold-cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4326, 2),\n",
       " (1000, 2),\n",
       " label\n",
       "  1    2822\n",
       " -1    1504\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf = pd.read_excel('./data/Task-2/train.xlsx')\n",
    "testdf=pd.read_excel('./data/Task-2/test.xlsx')\n",
    "#clean data\n",
    "traindf.drop_duplicates(subset='text',inplace=True)\n",
    "traindf.shape,testdf.shape,traindf['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       " 1    2822\n",
       "-1    2822\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "#class balancing\n",
    "ros = RandomOverSampler()\n",
    "train_x, train_y = ros.fit_resample(np.array(traindf['text']).reshape(-1, 1), np.array(traindf['label']).reshape(-1, 1))\n",
    "traindf_balance = pd.DataFrame(list(zip([x[0] for x in train_x], train_y)), columns = ['text', 'label'])\n",
    "traindf_balance['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "#data preprocessing\n",
    "#shuffle training dataset\n",
    "traindf=traindf_balance.sample(frac=1)\n",
    "traindf['label'].replace(-1, 0, inplace=True)\n",
    "\n",
    "testdf=testdf[['text']]\n",
    "\n",
    "testds_features=Features({'text': Value(dtype='string', id = None)})\n",
    "testds=Dataset.from_dict(mapping={\"text\": testdf['text'].to_list()},features=testds_features)\n",
    "\n",
    "\n",
    "trainds_features = Features({'text': Value(dtype='string', id = None), 'label': ClassLabel(num_classes=2 ,id=None)})\n",
    "trainds = Dataset.from_dict(mapping={\"text\": traindf['text'].to_list(), 'label': traindf['label'].to_list()},\n",
    "                            features=trainds_features)\n",
    "#whole training dataset\n",
    "trainds_org = trainds.shuffle(seed=42)\n",
    "\n",
    "\n",
    "#split\n",
    "cv_fold=5\n",
    "slice_ds=[]\n",
    "for i in range(cv_fold):\n",
    "    slice_ds.append(trainds_org.shard(num_shards=cv_fold,index=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"Intel/distilbert-base-uncased-finetuned-sst-2-english-int8-static\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modify epochs in training_args for real training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<optimum.onnxruntime.modeling_ort.ORTModelForSequenceClassification at 0x1b87583e650>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\",num_train_epochs=3,per_device_train_batch_size=8)\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = ORTModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15dc66222f243338bbff0491f66b348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5644 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a146859647f64e1296e39042f46a941c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1913af338f9047428350dc4ab9cc90fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a3113cbfaa4f7ca80cb798956838d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "use_io_binding was set to False, setting it to True because it can provide a huge speedup on GPUs. It is possible to disable this feature manually by setting the use_io_binding attribute back to False.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Asked to use CUDAExecutionProvider as an ONNX Runtime execution provider, but the available execution providers are ['AzureExecutionProvider', 'CPUExecutionProvider'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m tokenized_train_datasets \u001b[38;5;241m=\u001b[39m trainds\u001b[38;5;241m.\u001b[39mmap(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     36\u001b[0m tokenized_eval_datasets\u001b[38;5;241m=\u001b[39mvalds\u001b[38;5;241m.\u001b[39mmap(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 38\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     39\u001b[0m     model,\n\u001b[0;32m     40\u001b[0m     training_args,\n\u001b[0;32m     41\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_train_datasets,\n\u001b[0;32m     42\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_eval_datasets,\n\u001b[0;32m     43\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m     44\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m     45\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     46\u001b[0m )\n\u001b[0;32m     48\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     49\u001b[0m val_result\u001b[38;5;241m=\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:504\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[0;32m    503\u001b[0m ):\n\u001b[1;32m--> 504\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_move_model_to_device(model, args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:728\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[1;34m(self, model, device)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[1;32m--> 728\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    729\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\optimum\\onnxruntime\\modeling_ort.py:325\u001b[0m, in \u001b[0;36mORTModel.to\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m    324\u001b[0m provider \u001b[38;5;241m=\u001b[39m get_provider_for_device(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 325\u001b[0m validate_provider_availability(provider)  \u001b[38;5;66;03m# raise error if the provider is not available\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mset_providers([provider], provider_options\u001b[38;5;241m=\u001b[39m[provider_options])\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproviders \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_providers()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\optimum\\onnxruntime\\utils.py:255\u001b[0m, in \u001b[0;36mvalidate_provider_availability\u001b[1;34m(provider)\u001b[0m\n\u001b[0;32m    253\u001b[0m available_providers \u001b[38;5;241m=\u001b[39m ort\u001b[38;5;241m.\u001b[39mget_available_providers()\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m provider \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m available_providers:\n\u001b[1;32m--> 255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to use \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovider\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as an ONNX Runtime execution provider, but the available execution providers are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_providers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Asked to use CUDAExecutionProvider as an ONNX Runtime execution provider, but the available execution providers are ['AzureExecutionProvider', 'CPUExecutionProvider']."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import Trainer\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_train_org_datasets=trainds_org.map(tokenize_function, batched=True)\n",
    "tokenized_test_datasets=testds.map(tokenize_function, batched=True)\n",
    "#cross_validation cv_fold=5\n",
    "total_f1_score=0\n",
    "for i in range(cv_fold):\n",
    "    valds=slice_ds[i]\n",
    "    if i==0:\n",
    "        trainds=concatenate_datasets(slice_ds[i+1:cv_fold])\n",
    "    elif i==cv_fold-1:\n",
    "        trainds=concatenate_datasets(slice_ds[0:i])\n",
    "    else:\n",
    "        temp=slice_ds[0:i]\n",
    "        for j in range(i+1,cv_fold):\n",
    "            temp.append(slice_ds[j])\n",
    "        trainds=concatenate_datasets(temp)\n",
    "\n",
    "\n",
    "    tokenized_train_datasets = trainds.map(tokenize_function, batched=True)\n",
    "    tokenized_eval_datasets=valds.map(tokenize_function, batched=True)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        training_args,\n",
    "        train_dataset=tokenized_train_datasets,\n",
    "        eval_dataset=tokenized_eval_datasets,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    val_result=trainer.evaluate()\n",
    "    total_f1_score+=val_result['eval_f1']\n",
    "#average f1 score\n",
    "f1_score_cv=total_f1_score/cv_fold\n",
    "f1_score_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1dae32f284b4b65a2cc7af5fd55c385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0329, 'learning_rate': 3.819641170915959e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0456, 'learning_rate': 2.639282341831917e-05, 'epoch': 1.42}\n",
      "{'loss': 0.0101, 'learning_rate': 1.4589235127478753e-05, 'epoch': 2.12}\n",
      "{'loss': 0.007, 'learning_rate': 2.785646836638338e-06, 'epoch': 2.83}\n",
      "{'train_runtime': 89.2388, 'train_samples_per_second': 189.738, 'train_steps_per_second': 23.734, 'train_loss': 0.022603803345378634, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a7d5718d2f43acbd4d4ec7a943da4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train on whole training datatest\n",
    "trainer_final = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_train_org_datasets,\n",
    "    # eval_dataset=tokenized_eval_datasets,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer_final.train()\n",
    "#predict on test dataset\n",
    "pred=trainer_final.predict(tokenized_test_datasets)\n",
    "#change logits to probability\n",
    "label_probability=torch.softmax(torch.tensor(pred[0]),1)\n",
    "#get labels\n",
    "labels=label_probability.argmax(axis=1).reshape(-1,1)\n",
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_to_submit=pd.read_excel('./data/Task-2/test_to-submit.xlsx')\n",
    "test_to_submit['label']=labels\n",
    "test_to_submit.loc[test_to_submit['label']==0,'label']=-1\n",
    "# test_to_submit\n",
    "test_to_submit.to_excel('./data/Task-2/test_to-submit_answers1.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_final.save_model('./disroberta_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'label': 'LABEL_0', 'score': 0.9999092817306519},\n",
       "  {'label': 'LABEL_1', 'score': 9.07236390048638e-05}]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = AutoModelForSequenceClassification.from_pretrained(\"./roberta_model\", )\n",
    "# from transformers import pipeline\n",
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "model = new_model\n",
    "# tokenizer = tokenizer\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "# outputs a list of dicts like [[{'label': 'NEGATIVE', 'score': 0.0001223755971295759},  {'label': 'POSITIVE', 'score': 0.9998776316642761}]]\n",
    "pipe(\"I hate this movie!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
